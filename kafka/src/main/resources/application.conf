collector {
  streams {
    sink {
      enabled = kafka
      threadPoolSize = 10
      retries = 10
      maxBytes = 1000000

      # Optional: SQS integration for redundancy or backup
      #sqs {
      #  # Mode: "mirror" (writes to both Kafka and SQS always) or "backup" (writes to SQS only when Kafka fails)
      #  mode = "mirror"  # Options: "mirror" or "backup"
      #
      #  region = "eu-central-1"
      #  threadPoolSize = 8
      #  startupCheckInterval = 10 seconds
      #  maxBufferSize = 100000
      #  goodQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/good-events"
      #  badQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/bad-events"
      #  aws {
      #    accessKey = iam
      #    secretKey = iam
      #  }
      #  backoffPolicy {
      #    minBackoff = 1000
      #    maxBackoff = 10000
      #    maxRetries = 10
      #  }
      #}
    }

    buffer {
      byteLimit = 3145728
      recordLimit = 500
      timeLimit = 5000
    }
   }
}


pekko {
  loglevel = WARNING
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
  logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"

  http.server {
    remote-address-header = on
    raw-request-uri-header = on

    parsing {
      max-uri-length = 32768
      uri-parsing-mode = relaxed
      illegal-header-warnings = off
    }

    max-connections = 2048
  }

  coordinated-shutdown {
    run-by-jvm-shutdown-hook = off
  }
}
