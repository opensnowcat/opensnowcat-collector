collector {
  streams {
    sink {
      enabled = kafka
      threadPoolSize = 10
      retries = 10
      maxBytes = 1000000

      # Kafka producer configuration (optional)
      # The kafka producer has a variety of possible configuration options defined at
      # https://kafka.apache.org/documentation/#producerconfigs
      # Some values are set to other values from this config by default:
      # "bootstrap.servers" = brokers
      # "buffer.memory"     = buffer.byteLimit
      # "linger.ms"         = buffer.timeLimit
      #producerConf {
      #  "enable.idempotence" = false
      #  "max.in.flight.requests.per.connection" = 1000
      #  "linger.ms" = 100
      #  "max.block.ms" = 5000
      #  "metadata.max.age.ms" = 60000
      #  "batch.size" = 100000
      #  "buffer.memory" = 128000000
      #  "max.request.size" = 64000000
      #  "compression.type" = lz4
      #  "metadata.recovery.strategy" = rebootstrap
      #}

      # Kafka timeout configurations to prevent blocking when Kafka is unavailable (optional)
      # These settings control how quickly the collector fails over to SQS backup
      # If not specified, defaults are used (all values in milliseconds)
      #kafkaTimeouts {
      #  # Maximum time (ms) to block when Kafka is unavailable before failover
      #  # Controls how long send() and partitionsFor() will block
      #  # Trade-offs:
      #  #   - Higher values: More resilient to brief network hiccups, but slower failover to SQS
      #  #   - Lower values: Faster failover, but may prematurely fail during transient issues
      #  # Recommendation: Set based on your network latency + typical Kafka recovery time
      #  # Default: 5000 (5 seconds) - suitable for most environments
      #  maxBlockMs = 5000
      #
      #  # Maximum time (ms) for a request to complete
      #  # Controls the timeout for individual produce requests
      #  # Should align with your Kafka broker's request.timeout.ms setting
      #  # Default: 5000 (5 seconds)
      #  requestTimeoutMs = 5000
      #
      #  # Maximum time (ms) for the entire delivery process (including retries)
      #  # Should be >= request.timeout.ms to allow for retries
      #  # Formula: request.timeout.ms + (retry_time * retries)
      #  # Default: 10000 (10 seconds)
      #  deliveryTimeoutMs = 10000
      #
      #  # Maximum age (ms) of metadata before forcing a refresh
      #  # Trade-offs:
      #  #   - Lower values: Detect Kafka failures faster, but more network overhead
      #  #   - Higher values: Less network overhead, but slower failure detection
      #  # Recommendation: Balance between failure detection speed and network efficiency
      #  # Default: 5000 (5 seconds)
      #  metadataMaxAgeMs = 5000
      #}

      # Optional: SQS integration for redundancy or backup
      #sqs {
      #  # Mode: "mirror" (writes to both Kafka and SQS always) or "backup" (writes to SQS only when Kafka fails)
      #  mode = "mirror"  # Options: "mirror" or "backup"
      #
      #  region = "eu-central-1"
      #  threadPoolSize = 8
      #  startupCheckInterval = 10 seconds
      #  maxBufferSize = 100000
      #  goodQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/good-events"
      #  badQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/bad-events"
      #  aws {
      #    accessKey = iam
      #    secretKey = iam
      #  }
      #  backoffPolicy {
      #    minBackoff = 1000
      #    maxBackoff = 10000
      #    maxRetries = 10
      #  }
      #}
    }

    buffer {
      byteLimit = 3145728
      recordLimit = 500
      timeLimit = 5000
    }
   }
}


pekko {
  loglevel = WARNING
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]
  logging-filter = "org.apache.pekko.event.slf4j.Slf4jLoggingFilter"

  http.server {
    remote-address-header = on
    raw-request-uri-header = on

    parsing {
      max-uri-length = 32768
      uri-parsing-mode = relaxed
      illegal-header-warnings = off
    }

    max-connections = 2048
  }

  coordinated-shutdown {
    run-by-jvm-shutdown-hook = off
  }
}
