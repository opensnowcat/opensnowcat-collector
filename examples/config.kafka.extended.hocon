# Copyright (c) 2013-2022 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (config.hocon.sample) contains a template with
# configuration options for the Scala Stream Collector.
#
# To use, copy this to 'application.conf' and modify the configuration options.

# 'collector' contains configuration options for the main Scala collector.
collector {
  # The collector runs as a web service specified on the following interface and port.
  interface = "0.0.0.0"
  port = 8080

  # optional SSL/TLS configuration
  ssl {
    enable = false
    # whether to redirect HTTP to HTTPS
    redirect = false
    port = 443
  }

  # The collector responds with a cookie to requests with a path that matches the 'vendor/version' protocol.
  # The expected values are:
  # - com.snowplowanalytics.snowplow/tp2 for Tracker Protocol 2
  # - r/tp2 for redirects
  # - com.snowplowanalytics.iglu/v1 for the Iglu Webhook
  # Any path that matches the 'vendor/version' protocol will result in a cookie response, for use by custom webhooks
  # downstream of the collector.
  # But you can also map any valid (i.e. two-segment) path to one of the three defaults.
  # Your custom path must be the key and the value must be one of the corresponding default paths. Both must be full
  # valid paths starting with a leading slash.
  # Pass in an empty map to avoid mapping.
  paths {
    # "/com.acme/track" = "/com.snowplowanalytics.snowplow/tp2"
    # "/com.acme/redirect" = "/r/tp2"
    # "/com.acme/iglu" = "/com.snowplowanalytics.iglu/v1"
  }

  # Configure the P3P policy header.
  p3p {
    policyRef = "/w3c/p3p.xml"
    CP = "NOI DSP COR NID PSA OUR IND COM NAV STA"
  }

  # Cross domain policy configuration.
  # If "enabled" is set to "false", the collector will respond with a 404 to the /crossdomain.xml
  # route.
  crossDomain {
    enabled = false
    # Domains that are granted access, *.acme.com will match http://acme.com and http://sub.acme.com
    domains = [ "*" ]
    # Whether to only grant access to HTTPS or both HTTPS and HTTP sources
    secure = true
  }

  # The collector returns a cookie to clients for user identification
  # with the following domain and expiration.
  cookie {
    enabled = true
    expiration = 365 days
    # Network cookie name
    name = sp
    # The domain is optional and will make the cookie accessible to other
    # applications on the domain. Comment out these lines to tie cookies to
    # the collector's full domain.
    # The domain is determined by matching the domains from the Origin header of the request
    # to the list below. The first match is used. If no matches are found, the fallback domain will be used,
    # if configured.
    # If you specify a main domain, all subdomains on it will be matched.
    # If you specify a subdomain, only that subdomain will be matched.
    # Examples:
    # domain.com will match domain.com, www.domain.com and secure.client.domain.com
    # client.domain.com will match secure.client.domain.com but not domain.com or www.domain.com
    #domains = [
        # "acme1.com" # e.g. "domain.com" -> any origin domain ending with this will be matched and domain.com will be returned
        # ... more domains
    #]
    # ... more domains
    # If specified, the fallback domain will be used if none of the Origin header hosts matches the list of
    # cookie domains configured above. (For example, if there is no Origin header.)
    #fallbackDomain = "acme1.com"
    secure = true
    httpOnly = true
    # The sameSite is optional. You can choose to not specify the attribute, or you can use `Strict`,
    # `Lax` or `None` to limit the cookie sent context.
    #   Strict: the cookie will only be sent along with "same-site" requests.
    #   Lax: the cookie will be sent with same-site requests, and with cross-site top-level navigation.
    #   None: the cookie will be sent with same-site and cross-site requests.
    sameSite = "None"
  }

  # If you have a do not track cookie in place, the Scala Stream Collector can respect it by
  # completely bypassing the processing of an incoming request carrying this cookie, the collector
  # will simply reply by a 200 saying "do not track".
  # The cookie name and value must match the configuration below, where the names of the cookies must
  # match entirely and the value could be a regular expression.
  doNotTrackCookie {
    enabled = false
    name = ""
    value = ""
  }

  # When enabled and the cookie specified above is missing, performs a redirect to itself to check
  # if third-party cookies are blocked using the specified name. If they are indeed blocked,
  # fallbackNetworkId is used instead of generating a new random one.
  cookieBounce {
    enabled = false
    # The name of the request parameter which will be used on redirects checking that third-party
    # cookies work.
    name = "n3pc"
    # Network user id to fallback to when third-party cookies are blocked.
    fallbackNetworkUserId = "00000000-0000-4000-A000-000000000000"
    # Optionally, specify the name of the header containing the originating protocol for use in the
    # bounce redirect location. Use this if behind a load balancer that performs SSL termination.
    # The value of this header must be http or https. Example, if behind an AWS Classic ELB.
    #forwardedProtocolHeader = "X-Forwarded-Proto"
  }

  # When enabled, redirect prefix `r/` will be enabled and its query parameters resolved.
  # Otherwise the request prefixed with `r/` will be dropped with `404 Not Found`
  # Custom redirects configured in `paths` can still be used.
  enableDefaultRedirect = false

  # Domains which are valid for collector redirects. If empty (the default) then redirects are
  # allowed to any domain.
  redirectDomains = [
    # "acme1.com"
  ]

  # When enabled, the redirect url passed via the `u` query parameter is scanned for a placeholder
  # token. All instances of that token are replaced withe the network ID. If the placeholder isn't
  # specified, the default value is `${SP_NUID}`.
  redirectMacro {
    enabled = false
  }

  # Customize response handling for requests for the root path ("/").
  # Useful if you need to redirect to web content or privacy policies regarding the use of this collector.
  rootResponse {
    enabled = false
    statusCode = 302
    headers = {
    }
    body = ""
  }

  # Configuration related to CORS preflight requests
  cors {
    # The Access-Control-Max-Age response header indicates how long the results of a preflight
    # request can be cached. -1 seconds disables the cache. Chromium max is 10m, Firefox is 24h.
    accessControlMaxAge = 60 minutes
  }

  streams {
    # Events which have successfully been collected will be stored in the good stream/topic
    good = "good"

    # Bad rows (https://docs.snowplowanalytics.com/docs/try-snowplow/recipes/recipe-understanding-bad-data/) will be stored in the bad stream/topic.
    # The collector can currently produce two flavours of bad row:
    #  - a size_violation if an event is larger that the Kinesis (1MB) or SQS (256KB) limits;
    #  - a generic_error if a request's querystring cannot be parsed because of illegal characters
    bad = "bad"

    # Whether to use the incoming event's ip as the partition key for the good stream/topic
    # Note: Nsq does not make use of partition key.
    useIpAddressAsPartitionKey = false

    # Enable the chosen sink by uncommenting the appropriate configuration
    sink {
      # Choose between kinesis, sqs, google-pub-sub, kafka, nsq, or stdout.
      # To use stdout, comment or remove everything in the "collector.streams.sink" section except
      # "enabled" which should be set to "stdout".
      enabled = kafka

      # The following are used to authenticate for the Amazon Fargate
      # If both are set to 'default', the default provider chain is used
      # (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
      # If both are set to 'iam', use AWS IAM Roles to provision credentials.
      # If both are set to 'env', use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
      aws {
        accessKey = iam
        secretKey = iam
      }

      # Or Kafka
      brokers = "localhost:9092,another.host:9092"
      ## Number of retries to perform before giving up on sending a record
      retries = 10
      # The kafka producer has a variety of possible configuration options defined at
      # https://kafka.apache.org/documentation/#producerconfigs
      # Some values are set to other values from this config by default:
      # "bootstrap.servers" = brokers
      # "buffer.memory"     = buffer.byteLimit
      # "linger.ms"         = buffer.timeLimit
      #producerConf {
      #  acks = all
      #  "key.serializer"     = "org.apache.kafka.common.serialization.StringSerializer"
      #  "value.serializer"   = "org.apache.kafka.common.serialization.StringSerializer"
      #}

      # Kafka timeout configurations to prevent blocking when Kafka is unavailable
      # These settings control how quickly the collector fails over to SQS backup (if configured)
      kafkaTimeouts {
        # Maximum time (ms) to block when Kafka is unavailable before failover
        # Controls how long send() and partitionsFor() will block
        # Trade-offs:
        #   - Higher values: More resilient to brief network hiccups, but slower failover to SQS
        #   - Lower values: Faster failover, but may prematurely fail during transient issues
        # Recommendation: Set based on your network latency + typical Kafka recovery time
        # Default: 5000 (5 seconds) - suitable for most environments
        maxBlockMs = 5000

        # Maximum time (ms) for a request to complete
        # Controls the timeout for individual produce requests
        # Should align with your Kafka broker's request.timeout.ms setting
        # Default: 5000 (5 seconds)
        requestTimeoutMs = 5000

        # Maximum time (ms) for the entire delivery process (including retries)
        # Should be >= request.timeout.ms to allow for retries
        # Formula: request.timeout.ms + (retry_time * retries)
        # Default: 10000 (10 seconds)
        deliveryTimeoutMs = 10000

        # Maximum age (ms) of metadata before forcing a refresh
        # Trade-offs:
        #   - Lower values: Detect Kafka failures faster, but more network overhead
        #   - Higher values: Less network overhead, but slower failure detection
        # Recommendation: Balance between failure detection speed and network efficiency
        # Default: 5000 (5 seconds)
        metadataMaxAgeMs = 5000
      }

      # Retry policy for Kafka operations
      # Controls exponential backoff behavior when Kafka writes fail
      # After retries are exhausted, the collector will:
      #   - Mark Kafka as unhealthy
      #   - Start background health check for recovery detection
      #   - Failover to SQS if configured (otherwise continue retrying)
      backoffPolicy {
        # Minimum backoff delay (ms) between retries
        # First retry will wait at least this long
        # Default: 500ms
        minBackoff = 500

        # Maximum backoff delay (ms) between retries
        # Backoff will not exceed this value
        # Default: 5000ms (5 seconds)
        maxBackoff = 5000

        # Maximum number of retries before giving up
        # After exhausting retries:
        #   - With SQS backup: Events go to SQS, Kafka health check starts
        #   - Without SQS: Kafka is marked unhealthy, events are dropped
        # Recommendation: Balance between resilience and latency
        #   - Higher values: More resilient to transient failures, but longer delays
        #   - Lower values: Faster failover to SQS, but less tolerance for brief outages
        # Default: 10 retries (approx 30-50 seconds total with exponential backoff)
        maxRetries = 10
      }

      # Interval for background health checks
      # When Kafka is marked unhealthy, a background thread periodically checks if Kafka has recovered
      # Uses lightweight Kafka Admin API (describeTopics) - does NOT send actual events
      # Once Kafka is detected as healthy, the collector automatically resumes sending to Kafka
      # Trade-offs:
      #   - Lower values: Faster recovery detection, but more frequent API calls
      #   - Higher values: Less overhead, but slower to detect Kafka recovery
      # Recommendation: 5-10 seconds for most environments
      # Default: 5 seconds
      startupCheckInterval = 5 seconds

      # Optional. Maximum number of bytes that a single record can contain.
      # If a record is bigger, a size violation bad row is emitted instead
      # Default: 1 MB
      maxBytes = 1000000

      # Optional: SQS integration for redundancy or backup
      # When enabled, events are written to SQS only when Kafka fails (failover/backup mode)
      #sqs {
      #  region = "eu-central-1"
      #  threadPoolSize = 8
      #  startupCheckInterval = 10 seconds
      #  maxBufferSize = 100000
      #
      #  # IMPORTANT: Must be FULL queue URLs (not just queue names)
      #  # Format: https://sqs.{region}.amazonaws.com/{account-id}/{queue-name}
      #  goodQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/good-events"
      #  badQueueUrl = "https://sqs.eu-central-1.amazonaws.com/123456789012/bad-events"
      #
      #  aws {
      #    accessKey = iam
      #    secretKey = iam
      #  }
      #  backoffPolicy {
      #    minBackoff = 1000
      #    maxBackoff = 10000
      #    maxRetries = 10
      #  }
      #}
    }

    # Incoming events are stored in a buffer before being sent to Kinesis/Kafka.
    # Note: Buffering is not supported by NSQ.
    # The buffer is emptied whenever:
    # - the number of stored records reaches record-limit or
    # - the combined size of the stored records reaches byte-limit or
    # - the time in milliseconds since the buffer was last emptied reaches time-limit
    buffer {
      # Maximum buffer size in bytes (3MB)
      # For high-volume production, consider increasing to 10485760 (10MB)
      byteLimit = 3145728

      # Maximum number of records before flush
      # For high-volume production, consider increasing to 1000
      recordLimit = 500

      # Maximum time to buffer before flush (5 seconds)
      # Balances latency vs. efficiency
      # Lower values = lower latency but more API calls
      # Higher values = higher latency but fewer API calls
      timeLimit = 5000
    }
  }

  # Telemetry sends heartbeat events to external pipeline.
  # Unless disable parameter set to true, this feature will be enabled. Deleting whole section will not disable it.
  # Schema URI: iglu:com.snowplowanalytics.oss/oss_context/jsonschema/1-0-1
  #
  telemetry {
      disable = false
      interval = 60 minutes

      # Connection properties for the receiving pipeline
      method = POST
      url = sp.snowcatcloud.com
      port = 443
      secure = true
   }
   # HTTP metrics reported by collector
   monitoring.metrics.statsd {
      enabled = false
      # StatsD metric reporting protocol configuration
      hostname = localhost
      port = 8125
      # Required, how frequently to report metrics
      period = "10 seconds"
      # Optional, override the default metric prefix
      # "prefix": "snowplow.collector"

      # Any key-value pairs to be tagged on every StatsD metric
      "tags": {
        "app": collector
      }
  }

  # Configures how long the colletor should pause after receiving a sigterm before starting the graceful shutdown.
  # During this period the collector continues to accept new connections and respond to requests.
  preTerminationPeriod = 10 seconds

  # During the preTerminationPeriod, the collector can be configured to return 503s on the /health endpoint
  # Can be helpful for removing the collector from a load balancer's targets.
  preTerminationUnhealthy = false

  # The server's deadline for closing connections during graceful shutdown
  terminationDeadline = 10 seconds

  experimental {
    # Enable an experimental feature to send some "warm-up" requests to the collector's own /health endpoint during startup.
    # We have found from experiment this can cut down the number of 502s returned from a load balancer in front of the collector in Kubernetes deployments.
    # More details in https://github.com/snowplow/stream-collector/issues/249
    warmup {
      enable = false
      numRequests = 2000
      maxConnections = 2000
      maxCycles = 3
    }
    # Enable an experimental feature to expose the routes from Segment.io, meaning that analytics.js could use collector's backend.
    enableAnalyticsJsBridge = false
  }
}

# https://pekko.apache.org/docs/pekko/current/general/configuration.html
pekko {
  loglevel = WARNING # 'OFF' for no logging, 'DEBUG' for all logging.
  loggers = ["org.apache.pekko.event.slf4j.Slf4jLogger"]

  # https://pekko.apache.org/docs/pekko-http/current/configuration.html
  http.server {
    # To obtain the hostname in the collector, the 'remote-address' header
    # should be set. By default, this is disabled, and enabling it
    # adds the 'Remote-Address' header to every request automatically.
    remote-address-header = on

    raw-request-uri-header = on

    # Define the maximum request length (the default is 2048)
    parsing {
      max-uri-length = 32768
      uri-parsing-mode = relaxed
    }

    max-connections = 2048
  }
}
